{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return v\n",
    "    else:\n",
    "        return v/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Dimension of a = (m,n_h,n_w,n_c)   W = (f, f, n_c_prev, n_c)  b = (1, 1, 1, n_c)\n",
    "### hparams is a dict that will contain stride, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(input_array, pad):\n",
    "    X_pad =  np.pad(input_array, ((0,0), (pad, pad), (pad, pad), (0, 0)), 'constant',constant_values=(0,0))\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_prev, W, b):\n",
    "    s = np.multiply(a_prev, W)\n",
    "    Z = np.sum(s)\n",
    "    Z = Z + np.squeeze(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_step(a_prev, W, b, hparams):\n",
    "    stride = hparams[\"stride\"]\n",
    "    pad = hparams[\"pad\"]\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = a_prev.shape\n",
    "    (f, f, n_C, n_C_prev) = W.shape\n",
    "    \n",
    "    n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
    "    n_W = int((n_W_prev + 2*pad - f)/stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    A_pad_prev = zero_pad(a_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_pad_prev = A_pad_prev\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    v_start = h*stride\n",
    "                    v_end = v_start + f\n",
    "                    h_start = w*stride\n",
    "                    h_end = h_start + f\n",
    "                    \n",
    "                    a_slice_prev = a_pad_prev[m, v_start:v_end, h_start:h_end, :]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "                    \n",
    "    cache = (a_prev, W, b, hparams)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_layer(a_prev, hparams, mode = \"max\") : #default pooling is max\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = a_prev.shape\n",
    "    \n",
    "    f = hparams[\"f\"]\n",
    "    stride = hparams[\"stride\"]\n",
    "    \n",
    "    n_H = int(1 + (n_H_prev - f)/stride)\n",
    "    n_W = int(1 + (n_W_prev - f)/stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    v_start = h*stride\n",
    "                    v_end = v_start + f\n",
    "                    h_start = h*stride\n",
    "                    h_end = h_start + f\n",
    "                    \n",
    "                    a_slice_prev = a_prev[i, v_start:v_end, h_start:h_end, c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.average(a_slice_prev)\n",
    "                        \n",
    "    cache = (A_prev, hparams)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    (A_prev, W, b, hparams) = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparams[\"stride\"]\n",
    "    pad = hparams[\"pad\"]\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    A_pad_prev = zero_pad(A_prev, pad)\n",
    "    dA_pad_prev = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        a_pad_prev = A_pad_prev[i, :, :, :]\n",
    "        da_pad_prev = A_pad_prev[i, :, :, :]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    v_start = stride*h\n",
    "                    v_end = v_start + f\n",
    "                    h_start = stride*w\n",
    "                    h_end = h_start + f\n",
    "                    \n",
    "                    a_slice = a_pad_prev[v_start:v_end, h_start:h_end, :]\n",
    "                    da_pad_prev[v_start:v_end, h_start:h_end, :] += np.multiply(W[:, :, :, c], dZ[i, h, w, c])\n",
    "                    dW[:, :, :, c] = a_slice*dZ[i, h, w, c]\n",
    "                    db[:, :, :, c] = dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = dA_pad_prev[pad:-pad, pad:-pad, :]\n",
    "        \n",
    "        return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_backwards(dA, cache, mode = \"max\"):\n",
    "    (A_prev, hparams) = cache\n",
    "    \n",
    "    stride = hparams[\"stride\"]\n",
    "    f = hparams[\"f\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros((A.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i, :, :, :]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    v_start = h*stride\n",
    "                    v_end = V_start + f\n",
    "                    h_start = w*stride\n",
    "                    h_end = h_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        a_slice_prev = a_prev[v_start:v_end, h_start:h_end, c]\n",
    "                        mask = (a_slice_prev == np.max(a_slice_prev))\n",
    "                        dA_prev[i, v_start:v_end, h_start:h_end, c] += np.multiply(dA[i, h, w, c], mask)\n",
    "                    \n",
    "                    elif mode == \"average\":\n",
    "                        da = np.copy(dA[i, h, w, c])\n",
    "                        dA_prev[i, v_start:v_end, h_start:h_end, c] += np.full((f, f), da/(f + f))\n",
    "                        \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params_fc(layer_dims):\n",
    "    params_fc = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        params_fc[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        params_fc[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return params_fc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0), x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)), x\n",
    "    \n",
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x)), x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward_single_step(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward(X, params):\n",
    "    caches = []\n",
    "    A = np.copy(X)\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = fc_forward_single_step(A_prev, params[\"W\" + str(l)], params[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = fc_forward_single_step(A_prev, params[\"W\" + str(\"L\")], params[\"b\" + str(\"L\")], \"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    return (np.squeeze((-1/m)*np.sum(np.dot(Y, np.log(AL).T), np.dot(1-Y, (np.log(1-AL)).T))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(s):\n",
    "    s = s.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "def relu_backward(x):\n",
    "    x[x > 0] = 1\n",
    "    x[x <= 0] = 0\n",
    "    return x\n",
    "    \n",
    "def fc_backward_single_step(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)  # Making sure the shape are same\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)],\n",
    "    grads[\"dW\" + str(L-1)],\n",
    "    grads[\"db\" + str(L-1)] = fc_backward_single_step(dA, cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        grads[\"dA\" + str(l)],\n",
    "        grads[\"dW\" + str(l)],\n",
    "        grads[\"db\" + str(l)] = fc_backward_single_step(grads[\"dA\" + str(l+1)],current_cache, \"relu\")\n",
    "        \n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_fc(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    for l in range(L):\n",
    "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - (learning_rate*grads[\"dW\" + str(l+1)])\n",
    "        params[\"b\" + str(l+1)] = params[\"W\" + str(l+1)] - (learning_rate*grads[\"db\" + str(l+1)])\n",
    "    \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dimensions and plotting an example\n",
    "\n",
    "index = 232  # Can be any whole number less than 60,000\n",
    "print(x_train[index].shape)\n",
    "\n",
    "print(y_train[index])\n",
    "plt.imshow(x_train[index], cmap=\"Greys\", norm = colors.Normalize(vmin=0.0, vmax=1.0))\n",
    "\n",
    "# Making dimensions -> (number_of_examples, 28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, axis = 0).reshape(60000, 28, 28, 1)\n",
    "x_test = np.expand_dims(x_test, axis = 0 ).reshape(10000, 28, 28, 1)\n",
    "\n",
    "#Normalizing the arrays\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def le_net_model(x_train, y_train, learning_rate, params, hparams):\n",
    "    [f1, f2, p1, p2, b1, b2, b3, b4, w3, w4 ] = params\n",
    "    [n_C0, n_C1, n_C2] = hparams\n",
    "    \n",
    "    conv1, cache1 = conv_forward_step(x_train, (f1, f1, n_C0, n_C1), b1, {\"stride\" : 1, \"pad\" : 0})\n",
    "    a_conv1, conv1 = relu(conv1)\n",
    "    \n",
    "    pool1, cache2 = pooling_layer(a_conv1, {\"f\" : p1, \"stride\" : 1}, mode = \"average\")\n",
    "    \n",
    "    conv2, cache3 = conv_forward_step(pool1, (f2, f2, n_C1, n_C2), b2, {\"stride\" : 1, \"pad\" : 0})\n",
    "    a_conv2, conv2 = relu(conv2)\n",
    "    \n",
    "    pool2, cache4 = pooling_layer(a_conv2, {\"f\" : p2, \"stride\" : 1}, mode = \"average\")\n",
    "    \n",
    "    print(pool2.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_C0 = 1\n",
    "n_C1 = 6\n",
    "n_C2 = 16\n",
    "\n",
    "\n",
    "f1 = 5\n",
    "f2 = 5\n",
    "p1 = 2\n",
    "p2 = 2\n",
    "b1 = (1, 1, n_C0)\n",
    "b2 = (1, 1, n_C1)\n",
    "b3 = (1, 1, n_C2)\n",
    "b4 = (1, 1, n_C3)\n",
    "w3 = np.random.randn(2, 3)\n",
    "w4 = np.random.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
